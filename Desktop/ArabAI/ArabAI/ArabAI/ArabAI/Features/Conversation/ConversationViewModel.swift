//
//  ConversationViewModel.swift
//  ArabAI
//
//  Main conversation orchestration - connects all components
//

import Foundation
import Combine
import SwiftUI

/// Orchestrates conversation flow between audio, network, and avatar
class ConversationViewModel: ObservableObject {

    // MARK: - Published State

    @Published var isConversationActive = false
    @Published var statusMessage = "Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"  // "Ready for conversation"
    @Published var lastUserMessage = ""
    @Published var lastAIMessage = ""
    @Published var errorMessage: String?

    // UI feedback state
    @Published var connectionStatusArabic: String = "ØºÙŠØ± Ù…ØªØµÙ„"
    @Published var activityStatusArabic: String = ""

    // Learning mode (enabled by default)
    @Published var learningModeEnabled: Bool = true

    // Dialect selection
    @Published var selectedDialect: Dialect = .msa
    @Published var currentDialectName: String = "Ø§Ù„ÙØµØ­Ù‰"  // Arabic name display

    // MARK: - Components

    private let audioCaptureManager = AudioCaptureManager()
    private let audioPlayer = AudioPlayer()
    let webSocketClient: WebSocketClient  // Internal pour accÃ¨s depuis la View
    let avatarAnimator = AvatarAnimator()

    // MARK: - Internal State

    private var isAISpeaking = false

    // MARK: - Configuration

    private let backendURL: String

    // MARK: - Initialization

    init(backendURL: String = "ws://localhost:8000") {
        self.backendURL = backendURL
        self.webSocketClient = WebSocketClient(baseURL: backendURL)

        setupCallbacks()
    }

    // MARK: - Setup

    private func setupCallbacks() {
        // Audio capture callbacks
        audioCaptureManager.onTranscriptionUpdate = { [weak self] text, isFinal in
            guard let self = self else { return }

            DispatchQueue.main.async {
                self.lastUserMessage = text

                // Only send final transcriptions to reduce noise
                if isFinal {
                    self.statusMessage = "Ù…Ø¹Ø§Ù„Ø¬Ø©..."  // "Processing..."
                }
            }
        }

        audioCaptureManager.onSpeechSegmentComplete = { [weak self] text in
            guard let self = self else { return }

            DispatchQueue.main.async {
                print("ğŸ“¤ Sending final transcription: \(text)")
                self.webSocketClient.sendUserSpeech(text: text, isFinal: true)
            }
        }

        audioCaptureManager.onError = { [weak self] error in
            self?.handleError("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØª: \(error.localizedDescription)")
        }

        // WebSocket connection state callback
        webSocketClient.onConnectionStateChanged = { [weak self] state in
            self?.updateConnectionStatus(state)
        }

        // WebSocket callbacks
        webSocketClient.onConnected = { [weak self] in
            DispatchQueue.main.async {
                self?.statusMessage = "Ù…ØªØµÙ„"  // "Connected"
            }
        }

        webSocketClient.onDisconnected = { [weak self] in
            DispatchQueue.main.async {
                self?.statusMessage = "ØºÙŠØ± Ù…ØªØµÙ„"  // "Disconnected"
            }
        }

        webSocketClient.onAvatarStateChanged = { [weak self] state in
            DispatchQueue.main.async {
                guard let self = self else { return }

                self.avatarAnimator.setState(state)
                self.isAISpeaking = (state == .speaking)
                self.updateActivityStatus(state)

                // Pause/resume microphone transcription based on AI state
                switch state {
                case .idle:
                    self.statusMessage = "Ø¬Ø§Ù‡Ø²"  // "Ready"
                    // Don't resume here - will resume when audio finishes playing
                case .listening:
                    self.statusMessage = "Ø§Ø³ØªÙ…Ø§Ø¹..."  // "Listening..."
                    // Reset AI message for new streaming response
                    self.lastAIMessage = ""
                    // Only resume if not playing audio
                    if !self.isAISpeaking {
                        self.audioCaptureManager.resumeTranscription()
                    }
                case .speaking:
                    self.statusMessage = "ØªØ­Ø¯Ø«..."  // "Speaking..."
                    // Pause transcription to prevent AI voice from being transcribed
                    self.audioCaptureManager.pauseTranscription()
                }
            }
        }

        // User speech interruption callback
        audioCaptureManager.onUserStartedSpeaking = { [weak self] in
            guard let self = self else { return }

            // If AI is speaking, interrupt it
            if self.isAISpeaking {
                print("ğŸ›‘ User interrupted AI - stopping playback")
                self.audioPlayer.stopAll()
                self.isAISpeaking = false
            }
        }

        webSocketClient.onAIResponseText = { [weak self] text in
            DispatchQueue.main.async {
                guard let self = self else { return }

                // STREAMING MODE: Accumulate text chunks
                if self.lastAIMessage.isEmpty {
                    self.lastAIMessage = text
                } else {
                    self.lastAIMessage += " " + text
                }

                print("ğŸ’¬ AI chunk: \(text)")
                print("ğŸ’¬ Full message so far: \(self.lastAIMessage)")
            }
        }

        webSocketClient.onAIResponseAudio = { [weak self] audioData in
            // Enqueue audio chunk for playback
            self?.audioPlayer.enqueueAudioChunk(audioData)
        }

        webSocketClient.onDialectChanged = { [weak self] dialectCode, dialectNameArabic in
            DispatchQueue.main.async {
                self?.currentDialectName = dialectNameArabic
                print("âœ… Dialect changed to: \(dialectNameArabic)")
            }
        }

        webSocketClient.onError = { [weak self] error in
            self?.handleError("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: \(error.localizedDescription)")
        }

        // Audio player callbacks
        audioPlayer.onPlaybackStarted = { [weak self] in
            print("â–¶ï¸ Playback started")
        }

        audioPlayer.onPlaybackFinished = { [weak self] in
            print("â¹ Playback finished - resuming transcription")
            DispatchQueue.main.async {
                guard let self = self else { return }

                self.statusMessage = "Ø¬Ø§Ù‡Ø²"  // "Ready"
                self.isAISpeaking = false

                // NOW it's safe to resume transcription (audio has finished playing)
                self.audioCaptureManager.resumeTranscription()
            }
        }

        audioPlayer.onError = { [weak self] error in
            self?.handleError("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„: \(error.localizedDescription)")
        }
    }

    // MARK: - Conversation Control

    /// Start conversation
    func startConversation() {
        print("ğŸš€ Starting conversation")

        // Request permissions first
        audioCaptureManager.requestPermissions { [weak self] granted in
            guard let self = self else { return }

            DispatchQueue.main.async {
                if granted {
                    self.actuallyStartConversation()
                } else {
                    self.handleError("ÙŠØ±Ø¬Ù‰ Ù…Ù†Ø­ Ø§Ù„Ø¥Ø°Ù† Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª")
                    // "Please grant microphone and speech recognition permissions"
                }
            }
        }
    }

    private func actuallyStartConversation() {
        // Connect to backend
        webSocketClient.connect()

        // Wait a moment for connection
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            guard let self = self else { return }

            // Send start signal with learning mode and selected dialect
            self.webSocketClient.sendStartConversation(learningMode: self.learningModeEnabled, dialect: self.selectedDialect)

            // Start audio capture
            do {
                try self.audioCaptureManager.startRecording()
                self.isConversationActive = true
                self.statusMessage = "Ø§Ø³ØªÙ…Ø§Ø¹..."  // "Listening..."
                self.errorMessage = nil

                print("âœ… Conversation started (learning mode: \(self.learningModeEnabled))")
            } catch {
                self.handleError("ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: \(error.localizedDescription)")
            }
        }
    }

    /// Stop conversation
    func stopConversation() {
        print("â¹ Stopping conversation")

        // Stop audio capture
        audioCaptureManager.stopRecording()

        // Stop audio playback
        audioPlayer.stop()

        // Send stop signal and disconnect
        webSocketClient.sendStopConversation()

        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) { [weak self] in
            self?.webSocketClient.disconnect()
        }

        // Reset state
        isConversationActive = false
        statusMessage = "Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"  // "Ready for conversation"
        avatarAnimator.reset()

        print("âœ… Conversation stopped")
    }

    /// Toggle conversation on/off
    func toggleConversation() {
        if isConversationActive {
            stopConversation()
        } else {
            startConversation()
        }
    }

    /// Change dialect (works before and during conversation)
    func changeDialect(_ dialect: Dialect) {
        selectedDialect = dialect

        // If conversation is active, send change to backend
        if isConversationActive {
            webSocketClient.sendChangeDialect(dialect: dialect)
            print("ğŸ”„ Changing active dialect to: \(dialect.displayName)")
        } else {
            // Just update the selection for next conversation
            print("âœ… Selected dialect for next conversation: \(dialect.displayName)")
        }
    }

    // MARK: - Status Mapping

    /// Update connection status display in Arabic
    private func updateConnectionStatus(_ state: WebSocketClient.ConnectionState) {
        DispatchQueue.main.async {
            switch state {
            case .disconnected:
                self.connectionStatusArabic = "ØºÙŠØ± Ù…ØªØµÙ„"
            case .connecting:
                self.connectionStatusArabic = "Ø¬Ø§Ø±Ù Ø§Ù„Ø§ØªØµØ§Ù„..."
            case .connected:
                self.connectionStatusArabic = "Ù…ØªØµÙ„"
            case .reconnecting:
                self.connectionStatusArabic = "Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„..."
            }
        }
    }

    /// Update activity status display in Arabic
    private func updateActivityStatus(_ avatarState: AvatarState) {
        DispatchQueue.main.async {
            switch avatarState {
            case .idle:
                self.activityStatusArabic = ""
            case .listening:
                self.activityStatusArabic = "Ø£Ø³ØªÙ…Ø¹..."
            case .speaking:
                self.activityStatusArabic = "Ø£ØªÙƒÙ„Ù…..."
            }
        }
    }

    // MARK: - Error Handling

    private func handleError(_ message: String) {
        DispatchQueue.main.async {
            print("âŒ Error: \(message)")
            self.errorMessage = message
            self.statusMessage = "Ø®Ø·Ø£"  // "Error"

            // Auto-clear error after 5 seconds
            DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
                self.errorMessage = nil
            }
        }
    }

    // MARK: - Cleanup

    deinit {
        stopConversation()
    }
}
